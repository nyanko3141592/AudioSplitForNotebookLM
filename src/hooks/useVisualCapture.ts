import { useState, useCallback, useRef, useEffect } from 'react';
import type { VisualCaptureSettings, CaptureAnalysis, VisualCaptureState } from '../types/visualCapture';
import { defaultVisualCaptureSettings } from '../types/visualCapture';
import { VisualCaptureManager } from '../utils/visualCapture';
import { GeminiVisionAnalyzer, generateVisualBackgroundInfo } from '../utils/geminiVision';

const VISUAL_CAPTURE_SETTINGS_KEY = 'audioSplitter_visualCaptureSettings';

/**
 * ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
 */
const loadSettingsFromStorage = (): VisualCaptureSettings => {
  try {
    const stored = localStorage.getItem(VISUAL_CAPTURE_SETTINGS_KEY);
    if (stored) {
      const parsed = JSON.parse(stored);
      return { ...defaultVisualCaptureSettings, ...parsed };
    }
  } catch (error) {
    console.warn('Failed to load visual capture settings from localStorage:', error);
  }
  return defaultVisualCaptureSettings;
};

/**
 * ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«è¨­å®šã‚’ä¿å­˜
 */
const saveSettingsToStorage = (settings: VisualCaptureSettings): void => {
  try {
    localStorage.setItem(VISUAL_CAPTURE_SETTINGS_KEY, JSON.stringify(settings));
  } catch (error) {
    console.warn('Failed to save visual capture settings to localStorage:', error);
  }
};

export const useVisualCapture = () => {
  const [settings, setSettings] = useState<VisualCaptureSettings>(() => loadSettingsFromStorage());
  const [state, setState] = useState<VisualCaptureState>({
    isCapturing: false,
    capturedImages: [],
    nextCaptureIn: 0,
    totalCost: { imagesCount: 0, estimatedTokens: 0, estimatedCostJPY: 0, warning: null }
  });
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [analysisProgress, setAnalysisProgress] = useState({ current: 0, total: 0 });

  const captureManagerRef = useRef<VisualCaptureManager | null>(null);
  const visionAnalyzerRef = useRef<GeminiVisionAnalyzer | null>(null);

  /**
   * è¨­å®šã‚’æ›´æ–°
   */
  const updateSettings = useCallback((newSettings: Partial<VisualCaptureSettings>) => {
    setSettings(prev => {
      const updatedSettings = { ...prev, ...newSettings };
      saveSettingsToStorage(updatedSettings);
      return updatedSettings;
    });
  }, []);

  /**
   * API ã‚­ãƒ¼ã‚’è¨­å®š
   */
  const setApiCredentials = useCallback((apiKey: string, apiEndpoint?: string) => {
    if (apiKey) {
      visionAnalyzerRef.current = new GeminiVisionAnalyzer(
        apiKey, 
        apiEndpoint || 'https://generativelanguage.googleapis.com',
        'gemini-2.0-flash-lite',
        settings.duplicateThreshold,
        settings.duplicateDetection
      );
    } else {
      visionAnalyzerRef.current = null;
    }
  }, [settings.duplicateThreshold, settings.duplicateDetection]);

  /**
   * ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’é–‹å§‹
   */
  const startCapture = useCallback(async (stream: MediaStream) => {
    if (!settings.enabled) {
      console.log('Visual capture is disabled');
      return;
    }

    console.log('ğŸ¥ Starting visual capture with settings:', settings);

    // æ—¢å­˜ã®ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒã‚ã‚Œã°åœæ­¢
    if (captureManagerRef.current) {
      captureManagerRef.current.stopCapture();
    }

    // æ–°ã—ã„ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆ
    captureManagerRef.current = new VisualCaptureManager(settings);

    // çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
    setState(prev => ({
      ...prev,
      isCapturing: true,
      capturedImages: [],
      nextCaptureIn: 0
    }));

    // ã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹
    await captureManagerRef.current.startCapture(
      stream,
      // ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
      (analysis: CaptureAnalysis) => {
        console.log('ğŸ“¸ New capture received:', analysis.id);
        setState(prev => ({
          ...prev,
          capturedImages: [...prev.capturedImages, analysis],
          totalCost: {
            imagesCount: prev.capturedImages.length + 1,
            estimatedTokens: (prev.capturedImages.length + 1) * 1000,
            estimatedCostJPY: Math.ceil((prev.capturedImages.length + 1) * 1000 * 0.004),
            warning: null
          }
        }));
      },
      // çŠ¶æ…‹å¤‰æ›´ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
      (isCapturing: boolean, nextCaptureIn: number) => {
        setState(prev => ({
          ...prev,
          isCapturing,
          nextCaptureIn
        }));
      }
    );
  }, [settings]);

  /**
   * ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’åœæ­¢
   */
  const stopCapture = useCallback(() => {
    console.log('ğŸ›‘ Stopping visual capture');
    
    if (captureManagerRef.current) {
      captureManagerRef.current.stopCapture();
      captureManagerRef.current = null;
    }

    setState(prev => ({
      ...prev,
      isCapturing: false,
      nextCaptureIn: 0
    }));
  }, []);

  /**
   * ã‚­ãƒ£ãƒ—ãƒãƒ£ç”»åƒã‚’åˆ†æ
   */
  const analyzeCaptures = useCallback(async (): Promise<string> => {
    if (!visionAnalyzerRef.current) {
      throw new Error('Gemini API credentials not set');
    }

    if (state.capturedImages.length === 0) {
      return '';
    }

    const unanalyzedCaptures = state.capturedImages.filter(c => !c.description && !c.error && c.imageData);
    
    // If all captures are already analyzed, generate summary from existing data
    if (unanalyzedCaptures.length === 0) {
      const analyzedCaptures = state.capturedImages.filter(c => c.description && !c.error);
      if (analyzedCaptures.length > 0) {
        console.log('ğŸ¤– Generating summary from existing analyzed captures...');
        return await visionAnalyzerRef.current.generateSummary(analyzedCaptures);
      }
      return '';
    }

    console.log(`ğŸ¤– Starting two-stage analysis: ${unanalyzedCaptures.length} captures...`);
    
    setIsAnalyzing(true);
    setAnalysisProgress({ current: 0, total: unanalyzedCaptures.length });

    try {
      // Stage 1: Individual image analysis
      console.log('ğŸ“¸ Stage 1: Analyzing individual images...');
      const analyzedCaptures = await visionAnalyzerRef.current.analyzeBatch(
        unanalyzedCaptures,
        (current, total) => {
          setAnalysisProgress({ current, total });
        }
      );

      // Update state with individual analysis results
      setState(prev => {
        const updatedCaptures = prev.capturedImages.map(capture => {
          const analyzed = analyzedCaptures.find(a => a.id === capture.id);
          return analyzed || capture;
        });

        return {
          ...prev,
          capturedImages: updatedCaptures
        };
      });

      // Stage 2: Generate comprehensive summary
      console.log('ğŸ“ Stage 2: Generating comprehensive summary...');
      const validAnalyzedCaptures = analyzedCaptures.filter(c => c.description && !c.error);
      
      if (validAnalyzedCaptures.length === 0) {
        console.log('âš ï¸ No valid analyzed captures for summary generation');
        return '';
      }

      const summary = await visionAnalyzerRef.current.generateSummary(validAnalyzedCaptures);
      console.log('âœ… Two-stage visual analysis completed:', summary.substring(0, 100) + '...');
      
      return summary;

    } catch (error) {
      console.error('âŒ Visual analysis failed:', error);
      throw error;
    } finally {
      setIsAnalyzing(false);
      setAnalysisProgress({ current: 0, total: 0 });
    }
  }, [state.capturedImages]);

  /**
   * èƒŒæ™¯æƒ…å ±ã‚’ç”Ÿæˆï¼ˆåˆ†ææ¸ˆã¿ã‚­ãƒ£ãƒ—ãƒãƒ£ã‹ã‚‰ï¼‰
   */
  const generateBackgroundInfo = useCallback((): string => {
    return generateVisualBackgroundInfo(state.capturedImages);
  }, [state.capturedImages]);

  /**
   * æ‰‹å‹•ã§å³åº§ã«ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’å®Ÿè¡Œ
   */
  const captureNow = useCallback(async () => {
    if (!captureManagerRef.current) {
      console.warn('Capture manager not available for manual capture');
      return;
    }
    
    await captureManagerRef.current.captureNow();
  }, []);

  /**
   * ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚­ãƒ£ãƒ—ãƒãƒ£ã«è¿½åŠ 
   */
  const uploadImage = useCallback((file: File): Promise<void> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = (e) => {
        const imageData = e.target?.result as string;
        const analysis: CaptureAnalysis = {
          id: `upload_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
          timestamp: new Date().toISOString(),
          recordingTime: 0, // ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒã¯éŒ²éŸ³æ™‚é–“0ã¨ã™ã‚‹
          imageData,
          description: '',
          uploaded: true // ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒãƒ•ãƒ©ã‚°
        };

        setState(prev => ({
          ...prev,
          capturedImages: [...prev.capturedImages, analysis],
          totalCost: {
            imagesCount: prev.capturedImages.length + 1,
            estimatedTokens: (prev.capturedImages.length + 1) * 1000,
            estimatedCostJPY: Math.ceil((prev.capturedImages.length + 1) * 1000 * 0.004),
            warning: null
          }
        }));

        console.log('ğŸ“· Image uploaded and added to captures:', analysis.id);
        resolve();
      };
      reader.onerror = () => reject(new Error('Failed to read image file'));
      reader.readAsDataURL(file);
    });
  }, []);

  /**
   * ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’ã‚¯ãƒªã‚¢
   */
  const clearCaptures = useCallback(() => {
    setState(prev => ({
      ...prev,
      capturedImages: [],
      totalCost: { imagesCount: 0, estimatedTokens: 0, estimatedCostJPY: 0, warning: null }
    }));
  }, []);

  // ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¢ãƒ³ãƒã‚¦ãƒ³ãƒˆæ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  useEffect(() => {
    return () => {
      if (captureManagerRef.current) {
        captureManagerRef.current.stopCapture();
      }
    };
  }, []);

  return {
    // è¨­å®š
    settings,
    updateSettings,
    setApiCredentials,
    
    // çŠ¶æ…‹
    state,
    isAnalyzing,
    analysisProgress,
    
    // ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    startCapture,
    stopCapture,
    captureNow,
    uploadImage,
    analyzeCaptures,
    generateBackgroundInfo,
    clearCaptures,
    
    // è¨ˆç®—å€¤
    hasApiCredentials: visionAnalyzerRef.current !== null,
    canAnalyze: visionAnalyzerRef.current !== null && state.capturedImages.some(c => !c.description && !c.error && c.imageData),
  };
};